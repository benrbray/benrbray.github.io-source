<!DOCTYPE html>

<html>
<head>
    <!-- Metadata -->
	<meta charset="utf-8">

    <!-- Title -->
	<title>Benjamin R. Bray</title>

	<!-- Google Fonts -->
	<link href="https://fonts.googleapis.com/css?family=Droid+Sans|Droid+Sans+Mono|Droid+Serif" rel="stylesheet">
	<link href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,500,700" rel="stylesheet">
	<link href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700" rel="stylesheet">
	<!-- Site Styles -->
	<link rel="stylesheet" href="/theme/css/style.css" type="text/css">
	<link rel="stylesheet" href="/theme/css/pygments.css" type="text/css">
	<!-- KaTeX -->
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" integrity="sha384-9tPv11A+glH/on/wEu99NVwDPwkMQESOocs/ZGXPoIiLE8MU/qkqUcZ3zzL+6DuH" crossorigin="anonymous">
    <!-- The loading of KaTeX is deferred to speed up page rendering -->
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.js" integrity="sha384-U8Vrjwb8fuHMt6ewaCy8uqeUXv4oitYACKdB0VziCerzt011iQ/0TqlSlv8MReCm" crossorigin="anonymous"></script>
    <!-- To automatically render math in text elements, include the auto-render extension: -->
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/contrib/auto-render.min.js" integrity="sha384-aGfk5kvhIq5x1x5YdvCp4upKZYnA8ckafviDpmWEKp4afOZEqOli7gqSnh8I6enH" crossorigin="anonymous"
		onload='renderMathInElement(document.body, {
			delimiters : [
			{left: "$$", right: "$$", display: true},
			{left: "\\(", right: "\\)", display: false},
			{left: "$", right: "$", display: false},
			],
			macros : {
				"\\X" : "\\mathcal{X}",
				"\\L" : "\\mathcal{L}",
				"\\R" : "\\mathbb{R}",
			}
		});'></script>
</head>

<body>

<!-- CONTENT ------------------------------------------------------------------>

<!-- HEADER -->
<div id="header-box">
	<div id="header">
	<a id="header-name" href="/">
		Benjamin R. Bray
	</a><ul id="header-nav">
		<li><a href="/blog.html">WRITING</a></li>
		<li><a href="/projects.html">PROJECTS</a></li>
		<li><a href="/resources.html">RESOURCES</a></li>
		<li><a href="/resume.pdf">RESUME</a></li>
	</ul>
	</div>
</div>

<div id="page">
	
	<!-- CONTENT -->
	<div id="content">
<div class="center">

<h1 style="margin-bottom: 40px;">Writing</h1>

<!-- Article Loop ---------------------------------------------------------- -->
<div class="post-list">
    <div class="post-box">
        <a class="post-title" href="posts/2019/Aug/21/newtons-method-optimization-and-root-finding.html">Newton's Method, Optimization, and Root-finding</a>
        <span class="post-date">Wed, 21 Aug 2019.</span> <p>There are two versions of Newton's method, one for root-finding, <span class="math">\(f(x) = 0\)</span>, and one for optimization, <span class="math">\(min_{x \in \R^n} f(x)\)</span>.  In this post, I show explicitly that Newton's method for optimization is simply Newton's method applied to finding fixed points of the gradient map <span class="math">\(x \mapsto \nabla x f\)</span>.</p>
    </div>
    <div class="post-box">
        <a class="post-title" href="posts/2019/Aug/20/gradients-are-row-vectors-and-you-can-too.html">Gradients are Row Vectors (and you can too!)</a>
        <span class="post-date">Tue, 20 Aug 2019.</span> <p>Mathematicians tend to agree that gradients are row vectors, but for some reason computer scientists can't get on borad with the idea.  The goal of this post is to explain why gradients are most naturally expressed as row vectors, and to demonstrate the advantages of this perspective.</p>
    </div>
    <div class="post-box">
        <a class="post-title" href="posts/2018/May/02/algorithms-for-random-discrete-structures.html">Algorithms for Random Discrete Structures</a>
        <span class="post-date">Wed, 02 May 2018.</span> <p>Many applications require the random sampling of matrices with prescribed structure for modeling, statistical, or aesthetic purposes.  What does it mean for a random variable to be matrix-valued?  What can we say about the eigenvalues of a random matrix?  How can we design algorithms to sample from a target distribution on a group or manifold?  More generally, what can we say deterministic algorithms with random inputs?  Our study of random matrices will lead us to the <em>subgroup algorithm</em> (Diaconis 1987), which subsumes many familiar random sampling procedures.</p>
    </div>
    <div class="post-box">
        <a class="post-title" href="posts/2017/Jun/17/collision-detection-with-the-separating-axis-theorem.html">Collision Detection with the Separating Axis Theorem</a>
        <span class="post-date">Sat, 17 Jun 2017.</span> <p>In geometry, the hyperplane separation theorem is a theorem about disjoint convex sets in n-dimensional Euclidean space. There are several rather similar versions. In one version of the theorem, if both these sets are closed and at least one of them is compact, then there is a hyperplane in between them and even two parallel hyperplanes in between them separated by a gap. In another version, if both disjoint convex sets are open, then there is a hyperplane in between them, but not necessarily any gap. An axis which is orthogonal to a separating hyperplane is a separating axis, because the orthogonal projections of the convex bodies onto the axis are disjoint.</p>
    </div>
    <div class="post-box">
        <a class="post-title" href="posts/2015/Nov/26/expectation-maximization.html">Expectation Maximization</a>
        <span class="post-date">Thu, 26 Nov 2015.</span> <p>These notes provide a theoretical treatment of <strong>Expectation-Maximization</strong>, an iterative parameter estimation algorithm used to find local maxima of the likelihood function in the presence of hidden variables.  Introductory textbooks (MLAPP, PRML) typically state the algorithm without explanation and expect students to work blindly through derivations.  We find this approach to be unsatisfying, and instead choose to tackle the theory head-on, followed by plenty of examples.  Following (Neal &amp; Hinton 1998), we view expectation-maximization as coordinate ascent on the <strong>Evidence Lower Bound</strong>.  This perspective takes much of the mystery out of the algorithm and allows us to easily derive variants like <strong>Hard EM</strong> and <strong>Variational Inference</strong>.</p>
    </div>
    <div class="post-box">
        <a class="post-title" href="posts/2015/Aug/21/incompressible-fluid-simulation.html">Incompressible Fluid Simulation</a>
        <span class="post-date">Fri, 21 Aug 2015.</span> <p>Simulation of two-dimensional incompressible flow with periodic boundary conditions, achieved by solving the Euler Equations in the frequency domain via the Fast Fourier Transform (FFT).  Written in Java.</p>
    </div>
</div>

</div>
	</div>

	<!-- Page Footer -->
	<div id="page-footer">
		<span style="float: left">
			Powered by <a href="http://blog.getpelican.com/">Pelican</a> and hosted on <a href="https://github.com/benrbray/benrbray.github.io-source">GitHub</a>.
		</span>
		<span style="float: right">
			Last updated on October 16, 2019
		</span>
	</div>
</div>

</body>
</html>